{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWLB5nHmhNAdK+iLssTPDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Claudivansdn/Claudivansdn/blob/main/NeuralNetworkHidden.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX60RqcVWwNL",
        "outputId": "e6b70888-2606-4ab8-8b5c-e252a45196b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.35071884194749114\n",
            "Epoch 1000 - Loss: 0.24940113836971528\n",
            "Epoch 2000 - Loss: 0.2427528575727933\n",
            "Epoch 3000 - Loss: 0.1664500786226245\n",
            "Epoch 4000 - Loss: 0.033559643595896485\n",
            "Epoch 5000 - Loss: 0.012798119090837126\n",
            "Epoch 6000 - Loss: 0.00735186745873396\n",
            "Epoch 7000 - Loss: 0.0050320054987530866\n",
            "Epoch 8000 - Loss: 0.0037817160977144603\n",
            "Epoch 9000 - Loss: 0.0030101107704637265\n",
            "\n",
            "Saídas finais:\n",
            "[[0.05521003]\n",
            " [0.95292392]\n",
            " [0.95194693]\n",
            " [0.04886728]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Função Sigmoide e sua Derivada\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "##########################################\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, input_qtd, hidden_qtd, output_qtd):\n",
        "        # Inicializa os pesos e bias\n",
        "        self.weight_bias(input_qtd, hidden_qtd, output_qtd)\n",
        "\n",
        "  def weight_bias(self, input_qtd, hidden_qtd, output_qtd):\n",
        "        # Inicializa os pesos com valores aleatórios\n",
        "        self.weights_input_hidden = np.random.randn(input_qtd, hidden_qtd)\n",
        "        self.weights_output_hidden = np.random.randn(hidden_qtd, output_qtd)\n",
        "\n",
        "        # Inicializa os bias com valores aleatórios\n",
        "        self.bias_hidden = np.random.randn(1, hidden_qtd)\n",
        "        self.bias_output = np.random.randn(1, output_qtd)\n",
        "\n",
        "  def feedforward(self, x_input):\n",
        "        # Propagação da entrada para a camada oculta\n",
        "        self.input_hidden = np.dot(x_input, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.output_hidden = sigmoid(self.input_hidden)\n",
        "\n",
        "        # Propagação da camada oculta para a camada de saída\n",
        "        self.input_final = np.dot(self.output_hidden, self.weights_output_hidden) + self.bias_output\n",
        "        self.output_final = sigmoid(self.input_final)\n",
        "        return self.output_final\n",
        "\n",
        "  def backpropagate(self, x_input, y_output, learning_rate):\n",
        "        # Calcula o erro na saída\n",
        "        self.error = y_output - self.output_final\n",
        "\n",
        "        # Calcula o gradiente de erro para os pesos da camada de saída\n",
        "        delta_output = self.error * sigmoid_derivate(self.output_final)\n",
        "\n",
        "        # Calcula o erro da camada oculta\n",
        "        error_hidden = delta_output.dot(self.weights_output_hidden.T)\n",
        "\n",
        "        # Calcula o gradiente de erro para os pesos da camada oculta\n",
        "        delta_hidden = error_hidden * sigmoid_derivate(self.output_hidden)\n",
        "\n",
        "        # Atualiza os pesos e bias\n",
        "        self.weights_output_hidden += self.output_hidden.T.dot(delta_output) * learning_rate\n",
        "        self.weights_input_hidden += x_input.T.dot(delta_hidden) * learning_rate\n",
        "        self.bias_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
        "        self.bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "  def train(self, x_input, y_output, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Propagação para frente\n",
        "            output = self.feedforward(x_input)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagate(x_input, y_output, learning_rate)\n",
        "\n",
        "             # Exibição da Perda (Loss) a cada 1000 iterações\n",
        "            if epoch % 1000 == 0:\n",
        "               loss = np.mean(np.square(self.error))\n",
        "               print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "###################################################################\n",
        "\n",
        "# Conjunto de dados de entrada (XOR)\n",
        "x_input = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "# Saídas esperadas\n",
        "y_output = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Configurações de Treinamento\n",
        "input_qtd = 2 # numero de entradas\n",
        "hidden_qtd = 2  # numero de camadas ocultas\n",
        "output_qtd = 1  # numero de saídas\n",
        "\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "###################################################################\n",
        "\n",
        "# Colocar pra funcionar\n",
        "# Inicializa a rede neural\n",
        "rna = NeuralNetwork(input_qtd, hidden_qtd, output_qtd)\n",
        "\n",
        "# Treina a rede\n",
        "rna.train(x_input, y_output, learning_rate, epochs)\n",
        "\n",
        "# Resultados\n",
        "print(\"\\nSaídas finais:\")\n",
        "print(rna.feedforward(x_input))"
      ]
    }
  ]
}